# Image Processing

* [DENSELY CONNECTED CONVOLUTIONAL NETWORKS](http://arxiv.org/pdf/1608.06993v1.pdf)


# Reading

* [ICML 2016 Videos](http://techtalks.tv/icml/2016/tutorials/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue)
* [Deep Learing Summer School 2016, Montreal](http://videolectures.net/deeplearning2016_montreal/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue)


# Variational Auto Encoders

* [Unreasonable confusion of Variational AutoEncoders](https://jaan.io/unreasonable-confusion/)


# Graphical Model

* [Bishop Graphical Model 1](http://mlss.tuebingen.mpg.de/2013/bishop_slides.pdf)
* [Bishop Graphical Model 2](http://www.cs.ucf.edu/~mtappen/cap6412/lecs/graphical_models.pdf)
* [Machine Learning Summer School 2013](http://webdav.tuebingen.mpg.de/mlss2013/2013/speakers.html)


# Deep Learning School Day

* [Deep Learning School Day 1](https://www.youtube.com/watch?v=eyovmAtoUx0&feature=youtu.be)
* [Deep Learning School Day 2](https://www.youtube.com/watch?v=9dXiAecyJrY&feature=youtu.be)

* Back Propagation Derivation

* [Derviation of back propagation](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4)
* [Derivation of back propagation](http://briandolhansky.com/blog/2014/10/30/artificial-neural-networks-matrix-form-part-5)

# Reddit Pages
* [Machine Learning](https://www.reddit.com/r/MachineLearning/)
* [Deep Learning](https://www.reddit.com/r/deeplearning/)
* [Reddit Machine Learning Q&A Thread](https://www.reddit.com/user/feedtheaimbot)
* [ML Questions](https://www.reddit.com/r/MLQuestions/)

# [Visualize tSNE](http://distill.pub/2016/misread-tsne/)

# [ICASSP 2016](http://www.icassp2016.org/Papers/RegularProgram_MS.asp)

* EXPLORING MULTIDIMENSIONAL LSTMS FOR LARGE VOCABULARY ASR
* END-TO-END ATTENTION-BASED LARGE VOCABULARY SPEECH RECOGNITION
* DEEP CONVOLUTIONAL ACOUSTIC WORD EMBEDDINGS USING WORD-PAIR SIDE INFORMATION
* VERY DEEP MULTILINGUAL CONVOLUTIONAL NEURAL NETWORKS FOR LVCSR
* LISTEN, ATTEND AND SPELL: A NEURAL NETWORK FOR LARGE VOCABULARY CONVERSATIONAL SPEECH RECOGNITION

## [Adaptation](http://www.icassp2016.org/Papers/PublicSessionIndex3_MS.asp?Sessionid=1159)

* CONTEXT ADAPTIVE DEEP NEURAL NETWORKS FOR FAST ACOUSTIC MODEL ADAPTATION IN NOISY CONDITIONS
* ON COMBINING I-VECTORS AND DISCRIMINATIVE ADAPTATION METHODS FOR UNSUPERVISED SPEAKER NORMALIZATION IN DNN ACOUSTIC MODELS
* NON-NEGATIVE INTERMEDIATE-LAYER DNN ADAPTATION FOR A 10-KB SPEAKER ADAPTATION PROFILE
* SPEAKER CLUSTER-BASED SPEAKER ADAPTIVE TRAINING FOR DEEP NEURAL NETWORK ACOUSTIC MODELING
* EFFICIENT NON-LINEAR FEATURE ADAPTATION USING MAXOUT NETWORKS
* SEQUENCE SUMMARIZING NEURAL NETWORK FOR SPEAKER ADAPTATION

## Decoders

* PARALLELIZING WFST SPEECH DECODERS
* ACCELERATING MULTI-USER LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION ON HETEROGENEOUS CPU-GPU PLATFORMS

## [Neural Networks in Speech](http://www.icassp2016.org/Papers/PublicSessionIndex3_MS.asp?Sessionid=1160)

* FLAT START TRAINING OF CD-CTC-SMBR LSTM RNN ACOUSTIC MODELS
* PREDICTION-ADAPTATION-CORRECTION RECURRENT NEURAL NETWORKS FOR LOW-RESOURCE LANGUAGE SPEECH RECOGNITION
* A STUDY OF RANK-CONSTRAINED MULTILINGUAL DNNS FOR LOW-RESOURCE ASR
* MULTILINGUAL REGION-DEPENDENT TRANSFORMS
* EXPLOITING LSTM STRUCTURE IN DEEP NEURAL NETWORKS FOR SPEECH RECOGNITION
* SELF-STABILIZED DEEP NEURAL NETWORK


## [Language Models](http://www.icassp2016.org/Papers/PublicSessionIndex3_MS.asp?Sessionid=1165)

## [Robust Speech Recognition](http://www.icassp2016.org/Papers/PublicSessionIndex3_MS.asp?Sessionid=1158)

## RNN LM
* [Building an Efficient Neural langauge model](https://research.facebook.com/blog/building-an-efficient-neural-language-model-over-a-billion-words/)
* [n-gram interpolation with RNN LM](http://mi.eng.cam.ac.uk/~xc257/papers/ASRU2015-Interpolation.pdf)
> Methods to combine multiple language models had been studied and
compared in [27, 13, 28]. Most of these techniques are investigated
on n-gram LMs and their derivations, such as topic based n-gram
LM and cached based n-gram LM. RNNLMs are inherently different
from n-gram LMs in terms of their generalisation patterns. For this
reason, RNNLMs are usually linearly interpolated with n-gram LMs
to obtain both a good context coverage and strong generalisation [1,
3, 17, 18, 19, 20]. The interpolated LM probability is given by
P(wi|h
i−1
1
) = λPNG(wi|h
i−1
1
) + (1 − λ)PRN(wi|h
i−1
1
) (5)
λ is the global weight of the n-gram LM distribution PNG(·), which
can be optimized using the EM algorithm on a held-out set.

* [Short science, Hugo](http://www.shortscience.org/user?name=hlarochelle)
* [Recurrent Highway NEtworks](https://github.com/datavizweb/RecurrentHighwayNetworks)
* [Higherarchial multischale Recurrent Neural Network](http://openreview.net/pdf?id=S1di0sfgl)
* [Archive Sanity](http://www.arxiv-sanity.com/)
* [Building efficeint Neural Langauge model](https://research.facebook.com/blog/building-an-efficient-neural-language-model-over-a-billion-words/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue)




# On the fly Vocabulary addition reading list

* [Improved recognition of contact names in voice commands](https://www.google.co.in/search?q=%E2%80%9CImproved+recognition+of+contact+names+in+voice+commands&oq=%E2%80%9CImproved+recognition+of+contact+names+in+voice+commands&aqs=chrome..69i57j0.308j0j7&sourceid=chrome&es_sm=0&ie=UTF-8)
* [Dynamic Grammars with Lookahead Composition for WFST-based Speech Recognition](http://pallas.gavo.t.u-tokyo.ac.jp/~mine/paper/PDF/2012/INTERSPEECH_1272_t2012-9.pdf)
* [A Specialized WFST Approach for Class Models and Dynamic Vocabulary](http://www.lvcsr.com/static/pubs/dixon2012specialized.pdf)
* [Rapid Vocabulary Addition to Context-Dependent Decoder Graphs](http://www.isca-speech.org/archive/interspeech_2015/papers/i15_2112.pdf)
* [ISCA Online Archive](http://www.isca-speech.org/iscaweb/index.php/archive/online-archive)

# Improving the Language model
* [Bayesian Language Model Interpolation for Mobile Speech Input](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37567.pdf)

# ICLR 2017 Submission best papers
* [ICLR 2017 Submission discoveries](http://smerity.com/articles/2016/iclr_2017_submissions.html)
* [ICLR 2017 Submission discoveries NLP](https://amundtveit.com/2016/11/12/deep-learning-for-natural-language-processing-iclr-2017-discoveries/)


# ICLR 2017 Langauge Model Reading list

* [Data Noising as Smoothing in Neural Network Language Modelspdf](https://openreview.net/forum?id=H1VyHY9gg)
Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Lévy, Aiming Nie, Dan Jurafsky, Andrew Y. Ng
5 Nov 2016ICLR 2017 conference submissionreaders: everyone
Abstract: Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.

** Comments
** Read

* [DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning](https://openreview.net/forum?id=Bks8cPcxe)
Abstract: In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with 
fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.

In this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides 
(1) intuitive constructs to support compact encoding of deep networks; 
(2) symbolic gradient derivation of the networks; 
(3) static analysis for memory consumption and error detection; and 
(4) DSL-level optimization to improve memory and runtime efficiency. 

DeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on Nvidia GPU
via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. While the DeepDSL code is less than 100 lines for each of the network (e.g. Alexnet, GoogleNet, VGG, Overfeat, and Deep Residual Network), the compiled Java programs are highly efficient. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.

** Comment
** Pass

* [Frustratingly Short Attention Spans in Neural Language Modeling](https://openreview.net/forum?id=ByIAPUcee)
Michał Daniluk, Tim Rocktäschel, Johannes Welbl, Sebastian Riedel
5 Nov 2016ICLR 2017 conference submissionreaders: everyone
Abstract: Current language modeling architectures often use recurrent neural networks. Recently, various methods for incorporating differentiable memory into these architectures have been proposed. When predicting the next token, these models query information from a memory of the recent history and thus can facilitate learning mid- and long-range dependencies. However, conventional attention models produce a single output vector per time step that is used for predicting the next token as well as the key and value of a differentiable memory of the history of tokens. In this paper, we propose a key-value attention mechanism that produces separate representations for the key and value of a memory, and for a representation that encodes the next-word distribution. This usage of past memories outperforms existing memory-augmented neural language models on two corpora. Yet, we found that it mainly utilizes past memory only of the previous five representations. This led to the unexpected main finding that a much simpler model which simply uses a concatenation of output representations from the previous three-time steps is on par with more sophisticated memory-augmented neural language models.
TL;DR: We investigate various memory-augmented neural language models and compare them against state-of-the-art architectures.

* [Opening the vocabulary of neural language models with character-level word representations](https://openreview.net/forum?id=SygGlIBcel)
Matthieu Labeau, Alexandre Allauzen
4 Nov 2016ICLR 2017 conference submissionreaders: everyone
TL;DR:
Abstract: This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to circumvent the lack of vocabulary. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show promising results, with a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.

* [Reference-Aware Language Models](https://openreview.net/forum?id=ByG8A7cee)
Zichao Yang, Phil Blunsom, Chris Dyer, Wang Ling
4 Nov 2016ICLR 2017 conference submissionreaders: everyone
Abstract: We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or dis- course context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.

* [Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling](https://openreview.net/forum?id=r1aPbsFle)
Hakan Inan, Khashayar Khosravi, Richard Socher
4 Nov 2016ICLR 2017 conference submissionreaders: everyone
TL;DR:
Abstract: Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5.

* [A Neural Knowledge Language Model](https://openreview.net/forum?id=BJwFrvOeg)
Sungjin Ahn, Heeyoul Choi, Tanel Parnamaa, Yoshua Bengio
3 Nov 2016ICLR 2017 conference submissionreaders: everyone
Abstract: Current language models have significant limitations in their ability to encode and decode knowledge. This is mainly because they acquire knowledge based on statistical co-occurrences, even if most of the knowledge words are rarely observed named entities. In this paper, we propose a Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by a knowledge graph with the RNN language model. At each time step, the model predicts a fact on which the observed word is to be based. Then, a word is either generated from the vocabulary or copied from the knowledge graph. We train and test the model on a new dataset, WikiFacts. In experiments, we show that the NKLM significantly improves the perplexity while generating a much smaller number of unknown words. In addition, we demonstrate that the sampled descriptions include named entities which were used to be the unknown words in RNN language models.
TL;DR: A neural recurrent language model which can extract knowledge from a knowledge base to generate knowledge related words such as person names, locations, years, etc.

* [Using the Output Embedding to Improve Language Models](https://openreview.net/forum?id=SyBin3sxg)
Ofir Press, Lior Wolf
6 Nov 2016ICLR 2017 conference submissionreaders: everyone
TL;DR:
Abstract: We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. 

* [Intelligible Language Modeling with Input Switched Affine Networks](https://openreview.net/forum?id=H1MjAnqxg)
Jakob Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-dickstein, David Sussillo
5 Nov 2016ICLR 2017 conference submissionreaders: everyone
Abstract: The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question.  There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. 
As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.
TL;DR: Input Switched Affine Networks combine intelligibility with performance for character level language modeling.
 
* [Improving Neural Language Models with a Continuous Cache](https://openreview.net/forum?id=B184E5qee)
Edouard Grave, Armand Joulin, Nicolas Usunier
5 Nov 2016ICLR 2017 conference submissionreaders: everyone
TL;DR:
Abstract: We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.

* [Wav2Letter: an End-to-End ConvNet-based Speech Recognition System](https://openreview.net/forum?id=BkUDvt5gg)
Ronan Collobert, Christian Puhrsch, Gabriel Synnaeve
5 Nov 2016ICLR 2017 conference submissionreaders: everyone
Abstract: This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC (Graves et al., 2006) while being simpler. We show competitive results in word error rate on the Librispeech corpus (Panayotov et al., 2015) with MFCC features, and promising results from raw waveform.
TL;DR: We propose convnet models and new sequence criterions for training end-to-end letter-based speech systems.

* [Professor Forcing: A New Algorithm for Training Recurrent Networks](https://arxiv.org/pdf/1610.09038v1.pdf)
Abstract
The Teacher Forcing algorithm trains recurrent networks by supplying observed
sequence values as inputs during training and using the network’s own one-stepahead
predictions to do multi-step sampling. We introduce the Professor Forcing
algorithm, which uses adversarial domain adaptation to encourage the dynamics of
the recurrent network to be the same when training the network and when sampling
from the network over multiple time steps. We apply Professor Forcing to language
modeling, vocal synthesis on raw waveforms, handwriting generation, and image
generation. Empirically we find that Professor Forcing acts as a regularizer, improving
test likelihood on character level Penn Treebank and sequential MNIST.
We also find that the model qualitatively improves samples, especially when sampling
for a large number of time steps. This is supported by human evaluation of
sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are
discussed. We produce T-SNEs showing that Professor Forcing successfully makes
the dynamics of the network during training and sampling more similar.


## Some resources

* [NYU Statistical and Mathematical Methods](http://www.cims.nyu.edu/~cfgranda/pages/DSGA1002_fall16/index.html)
* [NYU MS Data Science Course](http://cds.nyu.edu/academics/ms-in-data-science/course-grid/)
* [Optimization and Computational Linear Algebra for Data Science](http://www.cims.nyu.edu/~bandeira/Fall2016.DS.GA.3001.03.OptCompLinAlgDS.html)
* [Natural Langauge Understanding with Distribution Representation](https://docs.google.com/document/d/1YS5QRvqMJVs9n3sK5fFjuldY7_vh42C5uUfxUGgL-Gc/edit)
* [Active Research areas in DL for NLP](https://docs.google.com/presentation/d/1O-Ics69y445aWuxQ_VW6SDvKT9BGl3ZXLLZDG9tUiUY/edit#slide=id.g13c41d274f_0_103)
* [Progress in Exponential language model](http://cs.nyu.edu/~allauzen/asr15/guest_lecture_3.pdf)
* [Advance topics in language model](http://cs.nyu.edu/~allauzen/asr15/guest_lecture_1.pdf)
* [asr nyc wfst](http://cs.nyu.edu/~allauzen/asr15/)
* [Statistical Natural Language Processing](http://cs.nyu.edu/courses/fall11/CSCI-GA.3033-001/)


## Interspeech Reading list
 * [MIT Reading group](https://drive.google.com/file/d/0BwrtKEdpd7B_VnRfQ3RsbkVqeVE/view)
